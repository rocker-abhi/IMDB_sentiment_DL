{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AIM : To Create the model for sentiment analysis using deep learning ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# implement ann \n",
    "from keras import Sequential\n",
    "from keras.layers import Dense, Embedding, LSTM, SimpleRNN, GRU, Dropout\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"IMDB Dataset.csv\") # importing the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = dataframe[dataframe['sentiment'] == 'positive'] # positive comments\n",
    "negetive_review = dataframe[dataframe['sentiment'] == 'negative'] # negetive comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training shape : (50000, 2) , testing shape : (0, 2)\n"
     ]
    }
   ],
   "source": [
    "# selecting 10000 rows for training\n",
    "\n",
    "training = pd.concat([positive_data.iloc[:45000], negetive_review.iloc[:45000]], axis=0)\n",
    "test = pd.concat([positive_data.iloc[45000:], negetive_review.iloc[45000:]], axis=0)\n",
    "\n",
    "print(f\"training shape : {training.shape} , testing shape : {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'negative'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words =  set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(text):\n",
    "    # function to remove the stopwords .\n",
    "\n",
    "    text = word_tokenize(text)\n",
    "    result = []\n",
    "\n",
    "    for i in text :\n",
    "        if i not in stop_words :\n",
    "            result.append(i)\n",
    "\n",
    "    return \" \".join(result)\n",
    "\n",
    "training['review'] = training['review'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_text(text):\n",
    "\n",
    "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)  # Keep only letters and spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Remove extra spaces\n",
    "    return text\n",
    "\n",
    "training['review'] = training['review'].apply(remove_unwanted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    25000\n",
       "negative    25000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training['review'] = training['review'].apply( lambda x : x.lower())\n",
    "training['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training['sentiment'] = training['sentiment'].apply(lambda x : 1 if 'positive' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_review(text):\n",
    "    if text == 'positive':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "training['sentiment'] = training['sentiment'].apply(transform_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching oz episode ll...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a wonderful little production br br the filmin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i thought wonderful way spend time hot summer ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei s love time money visually stunn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>probably alltime favorite movie story selfless...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  one reviewers mentioned watching oz episode ll...          1\n",
       "1  a wonderful little production br br the filmin...          1\n",
       "2  i thought wonderful way spend time hot summer ...          1\n",
       "4  petter mattei s love time money visually stunn...          1\n",
       "5  probably alltime favorite movie story selfless...          1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1532"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# max sentence length\n",
    "max(training['review'].apply(lambda x : len(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(max_features=1000)\n",
    "\n",
    "x = vectorizer.fit_transform(training['review']).toarray()\n",
    "y = np.array(training['sentiment'], dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train : (40000, 1000), y_train : (40000,), x_test : (10000, 1000), y_test : (10000,)\n"
     ]
    }
   ],
   "source": [
    "x_train, x_test , y_train, y_test = train_test_split(x, y, random_state=42, test_size=0.2)\n",
    "print(f'x_train : {x_train.shape}, y_train : {y_train.shape}, x_test : {x_test.shape}, y_test : {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implement model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "400/400 [==============================] - 3s 3ms/step - loss: 0.5001 - accuracy: 0.7565\n",
      "Epoch 2/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3609 - accuracy: 0.8507\n",
      "Epoch 3/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3382 - accuracy: 0.8636\n",
      "Epoch 4/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3246 - accuracy: 0.8705\n",
      "Epoch 5/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.3095 - accuracy: 0.8774\n",
      "Epoch 6/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.3033 - accuracy: 0.8798\n",
      "Epoch 7/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.2921 - accuracy: 0.8854\n",
      "Epoch 8/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2823 - accuracy: 0.8898\n",
      "Epoch 9/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.2746 - accuracy: 0.8924\n",
      "Epoch 10/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.2684 - accuracy: 0.8959\n",
      "Epoch 11/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.2669 - accuracy: 0.8971\n",
      "Epoch 12/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.2554 - accuracy: 0.9014\n",
      "Epoch 13/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.2516 - accuracy: 0.9040\n",
      "Epoch 14/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2480 - accuracy: 0.9060\n",
      "Epoch 15/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.2432 - accuracy: 0.9057\n",
      "Epoch 16/100\n",
      "400/400 [==============================] - 3s 7ms/step - loss: 0.2385 - accuracy: 0.9078\n",
      "Epoch 17/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.2355 - accuracy: 0.9111\n",
      "Epoch 18/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2331 - accuracy: 0.9092\n",
      "Epoch 19/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2324 - accuracy: 0.9125\n",
      "Epoch 20/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.2282 - accuracy: 0.9127\n",
      "Epoch 21/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.2237 - accuracy: 0.9144\n",
      "Epoch 22/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.2212 - accuracy: 0.9148\n",
      "Epoch 23/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2217 - accuracy: 0.9148\n",
      "Epoch 24/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2168 - accuracy: 0.9165\n",
      "Epoch 25/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2135 - accuracy: 0.9166\n",
      "Epoch 26/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2121 - accuracy: 0.9165\n",
      "Epoch 27/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2126 - accuracy: 0.9166\n",
      "Epoch 28/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2083 - accuracy: 0.9184\n",
      "Epoch 29/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2052 - accuracy: 0.9202\n",
      "Epoch 30/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2054 - accuracy: 0.9196\n",
      "Epoch 31/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2046 - accuracy: 0.9207\n",
      "Epoch 32/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1989 - accuracy: 0.9231\n",
      "Epoch 33/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1996 - accuracy: 0.9216\n",
      "Epoch 34/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1986 - accuracy: 0.9218\n",
      "Epoch 35/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1994 - accuracy: 0.9201\n",
      "Epoch 36/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.2048 - accuracy: 0.9200\n",
      "Epoch 37/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1960 - accuracy: 0.9215\n",
      "Epoch 38/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1945 - accuracy: 0.9212\n",
      "Epoch 39/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1968 - accuracy: 0.9216\n",
      "Epoch 40/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1936 - accuracy: 0.9219\n",
      "Epoch 41/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1917 - accuracy: 0.9233\n",
      "Epoch 42/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1929 - accuracy: 0.9220\n",
      "Epoch 43/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1878 - accuracy: 0.9228\n",
      "Epoch 44/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1881 - accuracy: 0.9247\n",
      "Epoch 45/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1877 - accuracy: 0.9247\n",
      "Epoch 46/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1904 - accuracy: 0.9237\n",
      "Epoch 47/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1872 - accuracy: 0.9240\n",
      "Epoch 48/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1817 - accuracy: 0.9250\n",
      "Epoch 49/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1826 - accuracy: 0.9261\n",
      "Epoch 50/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1849 - accuracy: 0.9243\n",
      "Epoch 51/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.1842 - accuracy: 0.9242\n",
      "Epoch 52/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1863 - accuracy: 0.9250\n",
      "Epoch 53/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1805 - accuracy: 0.9266\n",
      "Epoch 54/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1841 - accuracy: 0.9238\n",
      "Epoch 55/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1801 - accuracy: 0.9264\n",
      "Epoch 56/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1822 - accuracy: 0.9267\n",
      "Epoch 57/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1811 - accuracy: 0.9266\n",
      "Epoch 58/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1799 - accuracy: 0.9255\n",
      "Epoch 59/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1776 - accuracy: 0.9261\n",
      "Epoch 60/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1771 - accuracy: 0.9269\n",
      "Epoch 61/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1757 - accuracy: 0.9264\n",
      "Epoch 62/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1774 - accuracy: 0.9258\n",
      "Epoch 63/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1778 - accuracy: 0.9254\n",
      "Epoch 64/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1782 - accuracy: 0.9280\n",
      "Epoch 65/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1762 - accuracy: 0.9265\n",
      "Epoch 66/100\n",
      "400/400 [==============================] - 2s 4ms/step - loss: 0.1733 - accuracy: 0.9290\n",
      "Epoch 67/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1746 - accuracy: 0.9270\n",
      "Epoch 68/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1749 - accuracy: 0.9286\n",
      "Epoch 69/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1751 - accuracy: 0.9255\n",
      "Epoch 70/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1742 - accuracy: 0.9270\n",
      "Epoch 71/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1724 - accuracy: 0.9270\n",
      "Epoch 72/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1715 - accuracy: 0.9275\n",
      "Epoch 73/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1698 - accuracy: 0.9305\n",
      "Epoch 74/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1716 - accuracy: 0.9287\n",
      "Epoch 75/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1709 - accuracy: 0.9279\n",
      "Epoch 76/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1732 - accuracy: 0.9264\n",
      "Epoch 77/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1686 - accuracy: 0.9297\n",
      "Epoch 78/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1676 - accuracy: 0.9280\n",
      "Epoch 79/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1674 - accuracy: 0.9294\n",
      "Epoch 80/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1644 - accuracy: 0.9288\n",
      "Epoch 81/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1684 - accuracy: 0.9285\n",
      "Epoch 82/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1654 - accuracy: 0.9286\n",
      "Epoch 83/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1650 - accuracy: 0.9305\n",
      "Epoch 84/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1677 - accuracy: 0.9286\n",
      "Epoch 85/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1664 - accuracy: 0.9295\n",
      "Epoch 86/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1663 - accuracy: 0.9289\n",
      "Epoch 87/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1654 - accuracy: 0.9288\n",
      "Epoch 88/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1683 - accuracy: 0.9294\n",
      "Epoch 89/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1655 - accuracy: 0.9299\n",
      "Epoch 90/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1632 - accuracy: 0.9294\n",
      "Epoch 91/100\n",
      "400/400 [==============================] - 1s 3ms/step - loss: 0.1625 - accuracy: 0.9322\n",
      "Epoch 92/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1583 - accuracy: 0.9345\n",
      "Epoch 93/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1620 - accuracy: 0.9304\n",
      "Epoch 94/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1629 - accuracy: 0.9306\n",
      "Epoch 95/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1599 - accuracy: 0.9320\n",
      "Epoch 96/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1598 - accuracy: 0.9311\n",
      "Epoch 97/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1592 - accuracy: 0.9330\n",
      "Epoch 98/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1579 - accuracy: 0.9329\n",
      "Epoch 99/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1622 - accuracy: 0.9305\n",
      "Epoch 100/100\n",
      "400/400 [==============================] - 1s 4ms/step - loss: 0.1664 - accuracy: 0.9286\n",
      "ANN Accuracy: 0.8466\n"
     ]
    }
   ],
   "source": [
    "# ann model\n",
    "\n",
    "input_dim = x_train.shape[1]\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(16, activation='relu', input_dim=input_dim),\n",
    "    Dropout(0.3),  # Drops 30% of neurons\n",
    "    Dense(8, activation='relu'),\n",
    "    Dropout(0.3),  # Drops 30% of neurons\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(x_train, y_train, epochs=100, batch_size=100, verbose=1)\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"ANN Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lstm \n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=input_dim, output_dim=10, input_length=input_dim),\n",
    "    LSTM(32, return_sequences=True),\n",
    "    LSTM(16),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=20, verbose=1)\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"LSTM Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn \n",
    "\n",
    "model = Sequential([\n",
    "        Embedding(input_dim=input_dim, output_dim=10, input_length=input_dim),\n",
    "        SimpleRNN(16, return_sequences=True),\n",
    "        SimpleRNN(8),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=2000, verbose=1)\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"RNN Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gru \n",
    "\n",
    "model = Sequential([\n",
    "        Embedding(input_dim=input_dim, output_dim=10, input_length=input_dim),\n",
    "        GRU(16, return_sequences=True),\n",
    "        GRU(8),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "with tf.device('/GPU:0'):\n",
    "    model.fit(x_train, y_train, epochs=10, batch_size=200, verbose=1)\n",
    "    loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print(f\"GRU Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://47048aef-6e31-4a65-8d2f-f1406202ffec/assets\n"
     ]
    }
   ],
   "source": [
    "model.save('model.h5') # exporting model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting vectorizer \n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as f:\n",
    "    pkl.dump(vectorizer, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_deep_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
